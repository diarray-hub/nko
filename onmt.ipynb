{"cells":[{"cell_type":"markdown","metadata":{},"source":["### What is OpenNMT?\n","OpenNMT est un framework open-source pour la traduction machine basée sur les réseaux de neurones (Neural Machine translation).\\\n","Passer par un framework tel que openNMT est indispensable pour les débutants que nous somme, la conception d'une architecture pour un modèle et toutes les autres choses liées à la traduction machine en générale peuvent être complex.\n","\n","**Dans cette competition nous utiliserons l'implentation Pytorch de openNMT (OpenNMT-py)**"]},{"cell_type":"markdown","metadata":{"id":"ML_yvSbJ-dRC"},"source":["### Installing the packages\n","\n","En exécutant la cellule ci dessous vous procedez à l'installation d'OpenNMT-py (le main framework), de Pytorch (dont onmt a besoin) et de sentencepiece (utilisé par les Tokenizers)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T19:19:49.163060Z","iopub.status.busy":"2023-02-15T19:19:49.162708Z","iopub.status.idle":"2023-02-15T19:21:47.680508Z","shell.execute_reply":"2023-02-15T19:21:47.679134Z","shell.execute_reply.started":"2023-02-15T19:19:49.163029Z"},"id":"Tj5nn1CW9vJd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\n","Collecting OpenNMT-py\n","  Downloading OpenNMT_py-3.0.4-py3-none-any.whl (219 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.6/219.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\n","Collecting torch==1.12.1\n","  Downloading torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m934.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1) (4.1.1)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (23.0)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (9.0.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n","Requirement already satisfied: tensorboard>=2.3 in /opt/conda/lib/python3.7/site-packages (from OpenNMT-py) (2.9.1)\n","Collecting pyonmttok<2,>=1.35\n","  Downloading pyonmttok-1.36.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting waitress\n","  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rapidfuzz in /opt/conda/lib/python3.7/site-packages (from OpenNMT-py) (2.13.7)\n","Collecting configargparse\n","  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from OpenNMT-py) (6.0)\n","Collecting ctranslate2<4,>=3.2\n","  Downloading ctranslate2-3.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting sacrebleu\n","  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask in /opt/conda/lib/python3.7/site-packages (from OpenNMT-py) (2.2.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n","Collecting protobuf<3.20,>=3.9.2\n","  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.48.1)\n","Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (59.8.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (2.2.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n","Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from flask->OpenNMT-py) (3.1.2)\n","Requirement already satisfied: click>=8.0 in /opt/conda/lib/python3.7/site-packages (from flask->OpenNMT-py) (8.1.3)\n","Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from flask->OpenNMT-py) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.2.1)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu->OpenNMT-py) (0.4.6)\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from sacrebleu->OpenNMT-py) (4.9.1)\n","Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu->OpenNMT-py) (2.7.0)\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacrebleu->OpenNMT-py) (2021.11.10)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n","Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (1.16.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.7)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->flask->OpenNMT-py) (2.1.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.2.1)\n","Installing collected packages: waitress, torch, sacrebleu, pyonmttok, protobuf, ctranslate2, configargparse, OpenNMT-py\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.0\n","    Uninstalling torch-1.12.0:\n","      Successfully uninstalled torch-1.12.0\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tfx-bsl 1.10.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.61.0 which is incompatible.\n","tfx-bsl 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\n","tensorflow-transform 1.10.1 requires pyarrow<7,>=6, but you have pyarrow 9.0.0 which is incompatible.\n","tensorflow-serving-api 2.10.0 requires tensorflow<3,>=2.10.0, but you have tensorflow 2.9.2 which is incompatible.\n","ortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.19.6 which is incompatible.\n","onnx 1.13.0 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","nnabla 1.33.1 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-logging 3.2.2 requires google-cloud-core<3.0.0dev,>=2.0.0, but you have google-cloud-core 1.7.3 which is incompatible.\n","google-api-core 1.33.0 requires protobuf<4.0.0dev,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\n","gcsfs 2022.8.2 requires fsspec==2022.8.2, but you have fsspec 2023.1.0 which is incompatible.\n","apache-beam 2.41.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\n","apache-beam 2.41.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed OpenNMT-py-3.0.4 configargparse-1.5.3 ctranslate2-3.5.1 protobuf-3.19.6 pyonmttok-1.36.0 sacrebleu-2.3.1 torch-1.12.1 waitress-2.1.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","Collecting https://github.com/RobotsMali-AI/rmai/releases/download/0.0.4/rmaipkg-0.0.4.tar.gz\n","  Downloading https://github.com/RobotsMali-AI/rmai/releases/download/0.0.4/rmaipkg-0.0.4.tar.gz (10.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rmaipkg==0.0.4) (1.21.6)\n","Building wheels for collected packages: rmaipkg\n","  Building wheel for rmaipkg (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rmaipkg: filename=rmaipkg-0.0.4-py3-none-any.whl size=10693079 sha256=565ddf3b3c9fee2ffea1e5bd6274830c29c48764a02a6186441c301794141f0a\n","  Stored in directory: /root/.cache/pip/wheels/5b/05/e0/bdf99810c8faebdd495a0dfb90aa2a5936e0ff34cd1f394fa6\n","Successfully built rmaipkg\n","Installing collected packages: rmaipkg\n","Successfully installed rmaipkg-0.0.4\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install datasets OpenNMT-py sentencepiece sacrebleu torch==1.12.1"]},{"cell_type":"markdown","metadata":{},"source":["### Data preparation WORK\n","\n","Cette première étape consiste à préparer les données pour l'entraînement du modèle de traduction.\\\n","Ces données que nous allons utiliser sont structuré en \"lignes-parallèles\" dans divers fichier simplement nommées d'après la langue et l'usage qu'on à fera (eg, train.bam, test.fr et dev.bam) chacun ayant son correspondant parralèle, chaque ligne ayant sa traduction dans ce correspondant.\\\n","Avant de pouvoir entraîner un modèle, ces données doivent être \"néttoyé\" car ils contiennent très probablement des ligne mal structurée ou même vide, des doublons et autre type de \"noise\".\\\n","Enfin les données seront \"tokenizer\", cette étape consiste à encoder les mots en différent tokens qui sont des représentations scalaire des mots ou \"sous mots\" pour être précis, un token ne correspond pas nécéssairement à un mot mais on ne se préoccupera pas de cette étape dans ce notebook, OpenNMT va gérer ça pour nous.\n","\n","**Assurez vous que les répoertoires \"scripts\" et \"data\" sont présents**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T19:22:00.748887Z","iopub.status.busy":"2023-02-15T19:22:00.748502Z","iopub.status.idle":"2023-02-15T19:22:01.690531Z","shell.execute_reply":"2023-02-15T19:22:01.689254Z","shell.execute_reply.started":"2023-02-15T19:22:00.748853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","data/dev.bam         data/target.model  data/train.bam.fil.txt\n","data/dev.fr          data/target.vocab  data/train.fr\n","data/dev.sub.bam     data/test.bam      data/train.fr.fil.txt\n","data/dev.sub.fr      data/test.fr       data/train.sub.bam\n","data/robotsmali.tsv  data/test.sub.bam  data/train.sub.fr\n","data/source.model    data/test.sub.fr\n","data/source.vocab    data/train.bam\n"]}],"source":["ls data/ # lister le contenu du répertoire data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Cleaning task\n","!python scripts/filter.py data/train.fr data/train.bam"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training the Sentence piece subwording models for French and Bam\n","!python scripts/unigram.py data/train.fr.fil.txt data/train.bam.fil.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Deplacer les fichier créer par les différents scripts dans le dossier data\n","!mv *.vocab *.model data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Subwording the train, test and dev sets\n","!python scripts/subword.py data/source.model data/target.model data/train.fr.fil.txt data/train.bam.fil.txt\n","!python scripts/subword.py data/source.model data/target.model data/dev.fr data/dev.bam\n","!python scripts/subword.py data/source.model data/target.model data/test.fr data/test.bam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Take a look at the final training subwords that will be tokenized\n","!head -10 train.sub-src.txt train.sub-trg.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Move the new created files in folder \"data\" and change their names\n","!mv data/train.sub-src.txt data/train.sub.fr && mv data/train.sub-trg.txt data/train.sub.bam\n","!mv data/dev.sub-src.txt data/dev.sub.fr && mv data/dev.sub-trg.txt data/dev.sub.bam\n","!mv data/test.sub-src.txt data/test.sub.fr && mv data/test.sub-trg.txt data/test.sub.bam"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!wc data/*"]},{"cell_type":"markdown","metadata":{"id":"31VoxzGqHY8d"},"source":["### Model / Training Configuration\n","\n","La cellule ci dessous prend en charge la configuration des parametrès et hyperparamètres pour l'entrainement du modèle.\\\n","Sentez vous libre de modifier ces paramètres pour entrainer et observer les resultats sur le modèle"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T19:22:24.470938Z","iopub.status.busy":"2023-02-15T19:22:24.470487Z","iopub.status.idle":"2023-02-15T19:22:24.481038Z","shell.execute_reply":"2023-02-15T19:22:24.479918Z","shell.execute_reply.started":"2023-02-15T19:22:24.470899Z"},"id":"F6KKFqB0HeaH","trusted":true},"outputs":[],"source":["import os\n","\n","model_name = \"fr2bam\"\n","vocab_size = 50000\n","\n","training_steps = 25000\n","valid_steps = int(training_steps / 5)\n","save_ckpt_freq = valid_steps\n","warmup_steps = int(training_steps / 10)\n","reporting =  10 # int(training_steps/10)\n","\n","GPU = 1 # TOGGLE for GPU\n","\n","if(not os.path.exists(model_name)):\n","  os.makedirs(model_name)\n","\n","config = f\"\"\"\n","\n","# config.yaml\n","\n","\n","## Where the samples will be written\n","save_data: run\n","\n","# Training files\n","data:\n","    corpus_1:\n","        path_src: data/train.sub.fr\n","        path_tgt: data/train.sub.bam\n","        transforms: [filtertoolong] # change the transform method\n","    valid:\n","        path_src: data/dev.sub.fr\n","        path_tgt: data/dev.sub.bam\n","        transforms: [filtertoolong] # change the transform method\n","\n","# Vocabulary files, generated by onmt_build_vocab\n","src_vocab: models/{model_name}/run/source.vocab\n","tgt_vocab: models/{model_name}/run/target.vocab\n","\n","# Vocabulary size - should be the same as in sentence piece\n","src_vocab_size: 50000\n","tgt_vocab_size: 50000\n","\n","# Filter out source/target longer than n if [filtertoolong] enabled\n","src_seq_length: 150\n","src_seq_length: 150\n","\n","# Tokenization options\n","src_subword_model: source.model\n","tgt_subword_model: target.model\n","\n","# Where to save the log file and the output models/checkpoints\n","log_file: train.log\n","save_model: models/{model_name}\n","\n","# Stop training if it does not imporve after n validations\n","early_stopping: 3\n","\n","# Default: 5000 - Save a model checkpoint for each n\n","save_checkpoint_steps: {save_ckpt_freq}\n","\n","# To save space, limit checkpoints to last n\n","keep_checkpoint: 2\n","\n","seed: 1234\n","\n","# Default: 100000 - Train the model to max n steps \n","# Increase to 200000 or more for large datasets\n","# For fine-tuning, add up the required steps to the original steps\n","train_steps: {training_steps}\n","\n","# Default: 10000 - Run validation after n steps\n","valid_steps: {valid_steps}\n","\n","# Default: 4000 - for large datasets, try up to 8000\n","warmup_steps: {warmup_steps}\n","report_every: {reporting}\n","\n","# Batching\n","num_workers: 2  # Default: 2, set to 0 when RAM out of memory\n","batch_type: \"tokens\"\n","batch_size: 1024   # Tokens per batch, change when CUDA out of memory\n","valid_batch_size: 1024\n","max_generator_batches: 2\n","accum_count: [4]\n","accum_steps: [0]\n","\n","# Optimization\n","model_dtype: \"fp16\"\n","optim: \"adam\"\n","learning_rate: 2\n","decay_method: \"noam\"\n","adam_beta2: 0.998\n","max_grad_norm: 0\n","label_smoothing: 0.1\n","param_init: 0\n","param_init_glorot: true\n","normalization: \"tokens\"\n","\n","# Model\n","encoder_type: transformer\n","decoder_type: transformer\n","position_encoding: true\n","enc_layers: 6\n","dec_layers: 6\n","heads: 8\n","hidden_size: 1024\n","word_vec_size: 512 # investigate\n","transformer_ff: 2048\n","dropout_steps: [0]\n","dropout: [0.1]\n","attention_dropout: [0.1]\n","\n","\"\"\"\n","\n","if(GPU):\n","  config += \"\"\"\n","# Number of GPUs, and IDs of GPUs\n","world_size: 1\n","gpu_ranks: [0]\n","\"\"\"\n","\n","with open(f\"{model_name}/config.yaml\", \"w\") as fp:\n","  fp.write(config)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T19:22:50.217959Z","iopub.status.busy":"2023-02-15T19:22:50.217594Z","iopub.status.idle":"2023-02-15T19:22:51.153927Z","shell.execute_reply":"2023-02-15T19:22:51.152695Z","shell.execute_reply.started":"2023-02-15T19:22:50.217929Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","2\n"]}],"source":["!nproc --all"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T19:22:56.893496Z","iopub.status.busy":"2023-02-15T19:22:56.893086Z","iopub.status.idle":"2023-02-15T19:23:00.822261Z","shell.execute_reply":"2023-02-15T19:23:00.821037Z","shell.execute_reply.started":"2023-02-15T19:22:56.893459Z"},"id":"Tv8QoZU5TZ8l","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","[2023-02-15 19:22:58,804 INFO] Counter vocab from -1 samples.\n","[2023-02-15 19:22:58,804 INFO] n_sample=-1: Build vocab on full datasets.\n","[2023-02-15 19:23:00,369 INFO] * Transform statistics for corpus_1(50.00%):\n","\t\t\t* FilterTooLongStats(filtered=9)\n","\n","[2023-02-15 19:23:00,414 INFO] * Transform statistics for corpus_1(50.00%):\n","\t\t\t* FilterTooLongStats(filtered=5)\n","\n","[2023-02-15 19:23:00,534 INFO] Counters src:25554\n","[2023-02-15 19:23:00,534 INFO] Counters tgt:34695\n","Traceback (most recent call last):\n","  File \"/opt/conda/bin/onmt_build_vocab\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/opt/conda/lib/python3.7/site-packages/onmt/bin/build_vocab.py\", line 202, in main\n","    build_vocab_main(opts)\n","  File \"/opt/conda/lib/python3.7/site-packages/onmt/bin/build_vocab.py\", line 186, in build_vocab_main\n","    save_counter(src_counter, opts.src_vocab)\n","  File \"/opt/conda/lib/python3.7/site-packages/onmt/bin/build_vocab.py\", line 175, in save_counter\n","    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n","  File \"/opt/conda/lib/python3.7/site-packages/onmt/utils/misc.py\", line 47, in check_path\n","    raise IOError(f\"path {path} exists, stop.\")\n","OSError: path models/bam2fr/run/source.vocab exists, stop.\n"]}],"source":["!onmt_build_vocab -config fr2bam/config.yaml -n_sample -1 -num_threads 2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-15T19:23:14.895683Z","iopub.status.busy":"2023-02-15T19:23:14.895305Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n","[2023-02-15 19:23:16,689 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n","[2023-02-15 19:23:16,689 INFO] Parsed 2 corpora from -data.\n","[2023-02-15 19:23:16,689 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n","[2023-02-15 19:23:16,903 INFO] Building model...\n","[2023-02-15 19:23:20,825 INFO] NMTModel(\n","  (encoder): TransformerEncoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(25560, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding()\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","  )\n","  (decoder): TransformerDecoder(\n","    (embeddings): Embeddings(\n","      (make_embedding): Sequential(\n","        (emb_luts): Elementwise(\n","          (0): Embedding(34704, 512, padding_idx=1)\n","        )\n","        (pe): PositionalEncoding()\n","      )\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","    (transformer_layers): ModuleList(\n","      (0): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (1): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (2): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (3): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (4): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","      (5): TransformerDecoderLayer(\n","        (self_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (feed_forward): PositionwiseFeedForward(\n","          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n","          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n","          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (dropout_1): Dropout(p=0.1, inplace=False)\n","          (dropout_2): Dropout(p=0.1, inplace=False)\n","        )\n","        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        (drop): Dropout(p=0.1, inplace=False)\n","        (context_attn): MultiHeadedAttention(\n","          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n","          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n","          (softmax): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n","        )\n","        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (generator): Linear(in_features=512, out_features=34704, bias=True)\n",")\n","[2023-02-15 19:23:20,831 INFO] encoder: 31989760\n","[2023-02-15 19:23:20,831 INFO] decoder: 60772240\n","[2023-02-15 19:23:20,831 INFO] * number of parameters: 92762000\n","[2023-02-15 19:23:20,831 INFO]  * src vocab size = 25560\n","[2023-02-15 19:23:20,831 INFO]  * tgt vocab size = 34704\n","[2023-02-15 19:23:20,834 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 1\n","[2023-02-15 19:23:20,834 INFO] Starting training on GPU: [0]\n","[2023-02-15 19:23:20,835 INFO] Start training loop and validate every 5000 steps...\n","[2023-02-15 19:23:20,835 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n","[2023-02-15 19:23:22,758 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 2\n","[2023-02-15 19:23:25,460 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 3\n","[2023-02-15 19:23:27,364 INFO] Weighted corpora loaded so far:\n","\t\t\t* corpus_1: 4\n","[2023-02-15 19:23:43,097 INFO] Step 10/25000; acc: 2.5; ppl: 31345.1; xent: 10.4; lr: 0.00003; sents:    2218; bsz:  841/ 909/55; 1511/1634 tok/s;     22 sec;\n","[2023-02-15 19:23:46,696 INFO] Step 20/25000; acc: 6.0; ppl: 23667.5; xent: 10.1; lr: 0.00006; sents:    1809; bsz:  853/ 910/45; 9487/10119 tok/s;     26 sec;\n","[2023-02-15 19:23:50,314 INFO] Step 30/25000; acc: 6.0; ppl: 17307.2; xent: 9.8; lr: 0.00009; sents:    1859; bsz:  846/ 916/46; 9357/10126 tok/s;     29 sec;\n","[2023-02-15 19:23:53,949 INFO] Step 40/25000; acc: 6.4; ppl: 11134.0; xent: 9.3; lr: 0.00011; sents:    1891; bsz:  843/ 916/47; 9285/10077 tok/s;     33 sec;\n","[2023-02-15 19:23:57,557 INFO] Step 50/25000; acc: 7.4; ppl: 6470.7; xent: 8.8; lr: 0.00014; sents:    1999; bsz:  850/ 910/50; 9426/10089 tok/s;     37 sec;\n","[2023-02-15 19:24:01,132 INFO] Step 60/25000; acc: 5.9; ppl: 3598.0; xent: 8.2; lr: 0.00017; sents:    1618; bsz:  861/ 914/40; 9633/10230 tok/s;     40 sec;\n","[2023-02-15 19:24:04,699 INFO] Step 70/25000; acc: 7.8; ppl: 2142.1; xent: 7.7; lr: 0.00020; sents:    2331; bsz:  840/ 909/58; 9419/10200 tok/s;     44 sec;\n","[2023-02-15 19:24:08,282 INFO] Step 80/25000; acc: 7.8; ppl: 1492.3; xent: 7.3; lr: 0.00023; sents:    2004; bsz:  838/ 915/50; 9361/10214 tok/s;     47 sec;\n","[2023-02-15 19:24:11,913 INFO] Step 90/25000; acc: 8.7; ppl: 1362.6; xent: 7.2; lr: 0.00025; sents:    1642; bsz:  853/ 917/41; 9403/10108 tok/s;     51 sec;\n","[2023-02-15 19:24:15,479 INFO] Step 100/25000; acc: 9.8; ppl: 1212.4; xent: 7.1; lr: 0.00028; sents:    2274; bsz:  838/ 914/57; 9400/10254 tok/s;     55 sec;\n","[2023-02-15 19:24:19,072 INFO] Step 110/25000; acc: 11.6; ppl: 1246.3; xent: 7.1; lr: 0.00031; sents:    1756; bsz:  855/ 912/44; 9521/10161 tok/s;     58 sec;\n"]}],"source":["!onmt_train -config fr2bam/config.yaml -verbose"]},{"cell_type":"markdown","metadata":{"id":"kQHRzhIEZTRC"},"source":["## Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-qSMFQ0Za3T"},"outputs":[],"source":["!onmt_translate -model fr2bam/models/fr2bam_step_10000.pt -src data/test.sub.fr -output fr2bam/models/pred_10000.txt -gpu -1 -verbose"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45lJ9vy6y5w1"},"outputs":[],"source":["!python scripts/desubword.py data/target.model fr2bam/models/pred_10000.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47sp8qSWAt6q"},"outputs":[],"source":["# Sacrebleu testing CODE (calculating bleu and ter score)\n","bleu = !sacrebleu data/test.fr -i bam2fr/models/pred_10000.txt.desub.txt -m bleu -b -w 4\n","ter = !sacrebleu data/test.fr -i bam2fr/models/pred_10000.txt.desub.txt -m ter -b -w 4\n","\n","print(bleu)\n","print(ter)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM6b4yS0zDXa9ZLcWoPg/na","mount_file_id":"1Un-ObQG6c9zQN03nq4ugkDzCNOblwyve","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"3817ddb84c541c333c608c08e8d65a77181da8d4c3693ad20cf54437f03f9188"}}},"nbformat":4,"nbformat_minor":4}

{"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"3817ddb84c541c333c608c08e8d65a77181da8d4c3693ad20cf54437f03f9188"}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installations","metadata":{}},{"cell_type":"code","source":"%pip install OpenNMT-py sentencepiece sacrebleu torch","metadata":{"id":"D8gBg9ONY9cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clone the github repository that contains the scripts and the training data\n\n!git clone https://github.com/diarray-hub/nko.git\n\n# Change directory for this repo.\n%cd nko","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preparation WORK\n\nCette première étape consiste à préparer les données pour l'entraînement du modèle de traduction.\\\nCes données que nous allons utiliser sont structuré en \"lignes-parallèles\" dans divers fichier simplement nommées d'après la langue et l'usage qu'on à fera (eg, train.bam, test.fr et dev.bam) chacun ayant son correspondant parralèle, chaque ligne ayant sa traduction dans ce correspondant.\\\nAvant de pouvoir entraîner un modèle, ces données doivent être \"néttoyé\" car ils contiennent très probablement des ligne mal structurée ou même vide, des doublons et autre type de \"noise\".\\\nEnfin les données seront \"tokenizer\", cette étape consiste à encoder les mots en différent tokens qui sont des représentations scalaire des mots ou \"sous mots\" pour être précis, un token ne correspond pas nécéssairement à un mot mais on ne se préoccupera pas de cette étape dans ce notebook, OpenNMT va gérer ça pour nous.\n\n**Assurez vous que les répoertoires \"scripts\" et \"data\" sont présents**","metadata":{"id":"ML_yvSbJ-dRC"}},{"cell_type":"code","source":"!wget https://s3.amazonaws.com/opennmt-models/nllb-200/flores200_sacrebleu_tokenizer_spm.model -P data\n#!wget https://s3.amazonaws.com/opennmt-models/nllb-200/nllb-inference.yaml\n#!wget https://dl.fbaipublicfiles.com/large_objects/nllb/models/spm_200/dictionary.txt -P data\n#!wget https://s3.amazonaws.com/opennmt-models/nllb-200/nllb-200-600M-onmt.pt\n","metadata":{"id":"UhdA84QCD_-U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning task\n!python scripts/filter.py data/train.fr data/train.bam\n# Training the Sentence piece subwording models for French and Bam\n!python scripts/unigram.py data/train.fr.fil.txt data/train.bam.fil.txt\n# Deplacer les fichier créer par les différents scripts dans le dossier data\n!mv *.vocab *.model data","metadata":{"id":"4wfH0bTHDQYR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Subwording the data for inference\n!python scripts/subword.py data/source.model data/target.model data/50_test.fr data/50_test.bam\n# Change the names of the new created files in folder \"data\"\n!mv data/50_test.sub-src.txt data/50_test.sub.fr && mv data/50_test.sub-trg.txt data/50_test.sub.bam","metadata":{"id":"C52HNfyRDQYW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model / Training Configuration (skip this step for inference/ Sautez cette étape pour l'inference) ","metadata":{"id":"31VoxzGqHY8d"}},{"cell_type":"code","source":"# Try to avoid running out of memory\n#!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128","metadata":{"id":"1h408pBQY08H","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nmodel_name = \"fr2bam\"\nvocab_size = 50000\n\ntraining_steps = 5000\nvalid_steps = int(training_steps / 5)\nsave_ckpt_freq = int(training_steps / 10)\nwarmup_steps = int(training_steps / 10)\nreporting =  25 # int(training_steps/10)\nGPU = 1 # TOGGLE for GPU\n\nif(not os.path.exists(model_name)):\n  os.makedirs(model_name)\n\nconfig = f\"\"\"\n\n# config.yaml\n\n\n## Where the samples will be written\nsave_data: run\n\n# Training files\ndata:\n    corpus_1:\n        path_src: data/train.sub.fr\n        path_tgt: data/train.sub.bam\n        transforms: [sentencepiece] # try to tune the transform method (can find possible choice in the docs of OpenNMT-py)\n    valid:\n        path_src: data/dev.sub.fr\n        path_tgt: data/dev.sub.bam\n        transforms: [sentencepiece] # try to tune the transform method (can find possible choice in the docs of OpenNMT-py)\n\n# Vocabulary files, generated by onmt_build_vocab\nsrc_vocab: models/{model_name}/run/source.vocab\ntgt_vocab: models/{model_name}/run/target.vocab\nshare_vocab: true\n\ntrain_from: models/fr2bam_step_500.pt\n\n# Vocabulary size - should be the same as in sentence piece\nsrc_vocab_size: {vocab_size}\ntgt_vocab_size: {vocab_size}\n\n# Tokenization options\nsrc_subword_model: data/flores200_sacrebleu_tokenizer_spm.model\ntgt_subword_model: data/flores200_sacrebleu_tokenizer_spm.model\n\n# Where to save the log file and the output models/checkpoints\nlog_file: train.log\nsave_model: models/{model_name}\n\n# Stop training if it does not imporve after n validations\nearly_stopping: 3\n\n# Default: 5000 - Save a model checkpoint for each n\nsave_checkpoint_steps: {save_ckpt_freq}\n\n# To save space, limit checkpoints to last n\nkeep_checkpoint: 2\n\nseed: 3456\n\n# Default: 100000 - Train the model to max n steps \n# Increase to 200000 or more for large datasets\n# For fine-tuning, add up the required steps to the original steps\ntrain_steps: {training_steps}\n\n# Default: 10000 - Run validation after n steps\nvalid_steps: {valid_steps}\n\n# Default: 4000 - for large datasets, try up to 8000\nwarmup_steps: {warmup_steps}\nreport_every: {reporting}\n\n# Batching\nnum_workers: 2  # Default: 2, set to 0 when RAM out of memory\nbatch_type: \"tokens\"\nbatch_size: 512  # Tokens per batch, change when CUDA out of memory\nvalid_batch_size: 512\nmax_generator_batches: 2\naccum_count: [4]\naccum_steps: [0]\n\n# Optimization\nmodel_dtype: \"fp16\"\noptim: \"adam\"\nreset_optim: all\nlearning_rate: 0.0015\nnormalization: \"tokens\"\n\n# Model\ndropout_steps: [0]\ndropout: [0.1]\nattention_dropout: [0.1]\n\n\"\"\"\n\nif(GPU):\n  config += \"\"\"\nworld_size: 1\ngpu_ranks: [0]\n  \"\"\"\n\n#with open(f\"{model_name}/config.yaml\", \"w\") as fp:\n#  fp.write(config)","metadata":{"id":"F6KKFqB0HeaH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run inference","metadata":{"id":"kQHRzhIEZTRC"}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"id":"iOjn4RFgKspm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!onmt_translate -model fr2bam_step_500.pt -src data/50_test.sub.fr -output fr2bamNllb.pred_500.txt -gpu 0","metadata":{"id":"e-qSMFQ0Za3T","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show a preview of reference and predicted subwords \n!head data/50_test.sub.bam fr2bamNllb.pred_500.txt","metadata":{"id":"2yupj_BhdNNo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Desubword \n!python scripts/desubword.py data/flores200_sacrebleu_tokenizer_spm.model fr2bamNllb.pred_500.txt","metadata":{"id":"45lJ9vy6y5w1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show a preview of reference and predicted sentences \n!head fr2bamNllb.pred_500.txt.desub.txt data/test.bam","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}